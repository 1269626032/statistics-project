\documentclass{amsart}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{ctex}
\usepackage{wyz}

\title{线性回归分析}
\date{}
\author{mathwyz}

\begin{document}

\maketitle
\tableofcontents

\section{引论}

\subsection{什么是回归}
回归最早是由英国生物学家高尔顿在研究人类遗传问题时提出来的。
他发现一种有趣的现象，

\subsection{统计软件R}
\label{sec:02}

\subsubsection{R语言的发展历史}
\label{sec:0202}

\subsubsection{R语言的优势和缺点}
R语言与SAS,SPSS
1.首先R是GNU计划的一部分，是开源软件，是免费软件。
选择了开源软件，就选择了一套体系，它不仅仅是免费。
2.

R语言与Python

R语言与Hadoop家族
\subsubsection{R语言的网站}
\label{sec:0203}

目前,CRAN上面有15573个包。
\section{回归的方法}
\subsection{线性回归}
\subsection{线性与非线性}
数学家是这么一类人，他们把所有的问题都转化成线性代数的问题。
这是因为，现实生活中的很多问题可以用线性关系来替代。
比如动力系统，我们用线性部分来近似的替代整个动力系统，如果初始值在微小的范围内，解任然在微小的范围内，我们就说是稳定的。
这个时候就可以近似的用线性部分来替代非线性部分。
同样的，我们在统计中，经常也遇到这样的问题。
我们的变量之间是线性的，更准确的说是近似线性的。
那么我们就可以假设它是最简单的线性的相关关系。
我们就可以用线性的方法来近似。
另一方面，虽然很多问题并不是线性的，但是我们把变量经过变换之后，他们之间就会出现线性的相关关系，我们仍然可以用线性的来近似。

\begin{theorem}[Gauss-Markov定理]
  在线性无偏估计类中，最小二乘估计是唯一的具有最小方差的估计。
\end{theorem}

\subsection{逻辑回归}

\subsection{多项式回归}

\subsection{岭回归}

\subsubsection{复共线性}

\subsubsection{MSE}

\begin{theorem}
  在MSE的意义下存在领估计，它的MSE比线性回归要好．
\end{theorem}

\subsubsection{岭估计}

\subsubsection{计算岭回归的方法}
\begin{itemize}
\item Hoerl-Kennard公式
\item 岭迹法
\end{itemize}

岭回归在共线性数据分析中应用较多，也称为脊回归，它是一种有偏估计的回归方法，是在最小二乘估计法的基础上做了改进，通过舍弃最小二乘法的无偏性，使回归系数更加稳定和稳健。其中R方值会稍低于普通回归分析方法，但回归系数更加显著，主要用于变量间存在共线性和数据点较少时。
\subsection{最小角回归,Lasso回归}
LASSO回归的特点与岭回归类似，在拟合模型的同时进行变量筛选和复杂度调整。变量筛选是逐渐把变量放入模型从而得到更好的自变量组合。复杂度调整是通过参数调整来控制模型的复杂度，例如减少自变量的数量等，从而避免过拟合。LASSO回归也是擅长处理多重共线性或存在一定噪声和冗余的数据，可以支持连续型因变量、二元、多元离散变量的分析。
\subsubsection{理论基础}
\subsubsection{例子}
\subsubsection{相关的R的包}

\subsection{弹性回归}

\section{其他线性回国问题}

\subsection{异方差}
\subsubsection{具有异方差的线性回归模型}
\subsection{自相关}
\subsubsection{具有自回归的线性回归模型}


\subsection{过拟合}
\subsection{多重共线性}
\subsubsection{岭回归}
\subsubsection{主成分}
\subsubsection{LASSO}


\end{document}